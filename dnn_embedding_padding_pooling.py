# -*- coding: utf-8 -*-
"""DNN-embedding_padding_pooling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hm1NrziCf9mCJ7gpg48_x_ERUw7x4bhe
"""

import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import pandas as pd
import sklearn
import os
import sys
import time
import tensorflow as tf
from tensorflow import keras

print(tf.__version__)
print(sys.version_info)
for module in mpl,np,pd,sklearn,tf,keras:
    print(module.__name__,module.__version__)

# 取出来的词表从1开始，要进行处理
vocab_size=10000
# <3的id都是特殊字符
index_from=3
# 用keras里面的imdb数据集
imdb=keras.datasets.imdb
# 下载数据集，分为训练集、测试集
# num_words是词频，这里拿词频排名前10000的；取值为None,是保留所有单词
(train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=vocab_size,index_from=index_from)

# 输出是数值列表，要通过imdb_word_index.json对应到电影名字

# 训练集的大小
print(train_data.shape)
print(train_labels.shape)
# 训练集的第一个样本（是向量）
print(train_data[0],train_labels[0])
# 多维的数组 numpy.ndarray
print(type(train_data))
print(type(train_labels))
# train_labels的值0(negative),1(positive)-二分类
print(np.unique(train_labels))

#样本长度变长，后面需要处理为等长
print(len(train_data[0]),len(train_data[1]),len(train_data[100]))

# 测试集的大小
print(test_data.shape)
print(test_labels.shape)

# 下载词表，就是imdb_word_index.json
# key是单词，value是索引
word_index=imdb.get_word_index()
print(len(word_index))
print(type(word_index))

# word-level
print(word_index.get("footwork"))

# 取到的词表索引从1开始，id都偏移3
word_index={k:(v+3) for k,v in word_index.items()}
# 自定义索引0-3
word_index['<PAD>']=0 #填充字符
word_index['<START>']=1 #起始字符
word_index['<UNK>']=2 #找不到就返回UNK
word_index['<END>']=3 #结束字符

# 转成习惯的形式-索引是key,单词是value，就可以实现解码：把数字对应到一句单词组成的话
reverse_word_index=dict([(value,key) for key,value in word_index.items()])
print(type(reverse_word_index))
# 查看解码效果
print(reverse_word_index)


# 解码函数
def decode_review(text_ids):
  return " ".join([reverse_word_index.get(word_id,"<UNK>") for word_id in text_ids])

# 逐个对单词解码，得到一篇文本
decode_review(train_data[0])

print(reverse_word_index[34707])

print(word_index.get("footwork"))

# 设置输入词汇表的长度-长度<500会被补全，>500会被截断
max_length=500

# 填充padding
# value-用什么值去填充
# padding选择填充的顺序，2种-pre,post
train_data=keras.preprocessing.sequence.pad_sequences(train_data,value=word_index['<PAD>'],padding='post',maxlen=max_length)
# 测试集要和训练集结构相同，才能好好地应用
test_data=keras.preprocessing.sequence.pad_sequences(test_data,value=word_index['<PAD>'],padding='post',maxlen=max_length)

print(train_data[0])

# 一个单词的维度是16维
embedding_dim=16
batch_size=128

# 定义模型
# 定义矩阵[vocab_size,embedding_dim]
# GlobalAveragePooling1D 全局平均值池化-在max_length这个维度上做平均，就是1x16了
# 二分类问题，最后的激活函数用sigmoid
model=keras.models.Sequential([
             keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),
             keras.layers.GlobalAveragePooling1D(),
             keras.layers.Dense(64,activation='relu'),
             keras.layers.Dense(1,activation='sigmoid'),                               
])

model.summary()
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

# embedding:vocab_sizexembedding_dim=10000x16=160000
# dense 1088:wx+b embedding_dimx64+64 w,b都是64
# dense 65:wx+b wx1+1 w,b都是1

# 模型中的所有变量在训练过程中不断被调的参数
model.variables

# 数据集中只有训练集、测试集，没有验证集，就用validation_split-拿20%的训练集数据当作验证集数据
history=model.fit(train_data,train_labels,epochs=30,batch_size=batch_size,validation_split=0.2)

# 绘制学习曲线
def plot_learning_curves(history,label,epochs,min_value,max_value):
    data={}
    data[label]=history.history[label]
    data['val'+label]=history.history['val_'+label]
    pd.DataFrame(data).plot(figsize=(8,5))
    plt.grid(True)
    plt.axis([0,epochs,min_value,max_value])
    plt.show()

plot_learning_curves(history,'accuracy',30,0,1)
plot_learning_curves(history,'loss',30,0,1)

"""loss发生了过拟合"""

model.evaluate(
    test_data,test_labels,
    batch_size=batch_size,
    verbose=1
)