### Loss Function

机器学习中，我们通过最小化**Loss Function**从而找到最优的一组参数

**Loss Function**可以近似看成是预测值与真实值的**距离**

**0-1 Loss**

![](https://github.com/sherlcok314159/ML/blob/main/Images/loss_0_1.png)


**Square Error**

![](https://github.com/sherlcok314159/ML/blob/main/Images/square_error.png)


**Mean Square Error**
![](https://github.com/sherlcok314159/ML/blob/main/Images/mean_square_error.png)


**Root Mean Square Error**

![](https://github.com/sherlcok314159/ML/blob/main/Images/root_mean_square_error.png)


**Cross Entropy**

举**二元分类**的问题，则为伯努利分布，假设两个分布分别为**p,q**

![](https://github.com/sherlcok314159/ML/blob/main/Images/c1_distribution.png)


![](https://github.com/sherlcok314159/ML/blob/main/Images/bonuli_p.png)


![](https://github.com/sherlcok314159/ML/blob/main/Images/bonuli_q.png)

![](https://github.com/sherlcok314159/ML/blob/main/Images/cross_entropy.png)
