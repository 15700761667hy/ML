### Transformer

参考论文

https://arxiv.org/abs/1706.03762

https://arxiv.org/abs/1810.04805

**章节**
- [Reasons](#reasons)
- [Self-Attention](#self_attention)
    - [Multi-Head Attention](#multi)
- [Positional Encoding](#positional)
- [Add & Norm](#add)
- [Feed Forward](#feed)
- [Residual Dropout](#drop)
- [Encoder To Decoder](#etd)
- [Shared Weights](#share)
- [Effect](#effect)