
### Transformer

参考论文(https://arxiv.org/abs/1810.04805)

**章节**
- [Reasons](#reasons)
- [Self-Attention](#self_attention)
    - [Multi-Headed](#multi_headed)
- [Add & Normalize](#add)
- [Positional Encoding](#positional)

![](https://github.com/sherlcok314159/ML/blob/main/nlp/Images/transformer.png)


**<div id='reasons'>Reasons For Transformer</div>**